# -*- coding: utf-8 -*-
"""Full_Code_SuperKart_Model_Deployment_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ICgjV9gtxrLEenVD0MUxCZcEk43bMyUY

# **Problem Statement**

## Business Context

A sales forecast is a prediction of future sales revenue based on historical data, industry trends, and the status of the current sales pipeline. Businesses use the sales forecast to estimate weekly, monthly, quarterly, and annual sales totals. A company needs to make an accurate sales forecast as it adds value across an organization and helps the different verticals to chalk out their future course of action.

Forecasting helps an organization plan its sales operations by region and provides valuable insights to the supply chain team regarding the procurement of goods and materials. An accurate sales forecast process has many benefits which include improved decision-making about the future and reduction of sales pipeline and forecast risks. Moreover, it helps to reduce the time spent in planning territory coverage and establish benchmarks that can be used to assess trends in the future.

## Objective

SuperKart is a retail chain operating supermarkets and food marts across various tier cities, offering a wide range of products. To optimize its inventory management and make informed decisions around regional sales strategies, SuperKart wants to accurately forecast the sales revenue of its outlets for the upcoming quarter.

To operationalize these insights at scale, the company has partnered with a data science firm—not just to build a predictive model based on historical sales data, but to develop and deploy a robust forecasting solution that can be integrated into SuperKart’s decision-making systems and used across its network of stores.

## Data Description

The data contains the different attributes of the various products and stores.The detailed data dictionary is given below.

- **Product_Id** - unique identifier of each product, each identifier having two letters at the beginning followed by a number.
- **Product_Weight** - weight of each product
- **Product_Sugar_Content** - sugar content of each product like low sugar, regular and no sugar
- **Product_Allocated_Area** - ratio of the allocated display area of each product to the total display area of all the products in a store
- **Product_Type** - broad category for each product like meat, snack foods, hard drinks, dairy, canned, soft drinks, health and hygiene, baking goods, bread, breakfast, frozen foods, fruits and vegetables, household, seafood, starchy foods, others
- **Product_MRP** - maximum retail price of each product
- **Store_Id** - unique identifier of each store
- **Store_Establishment_Year** - year in which the store was established
- **Store_Size** - size of the store depending on sq. feet like high, medium and low
- **Store_Location_City_Type** - type of city in which the store is located like Tier 1, Tier 2 and Tier 3. Tier 1 consists of cities where the standard of living is comparatively higher than its Tier 2 and Tier 3 counterparts.
- **Store_Type** - type of store depending on the products that are being sold there like Departmental Store, Supermarket Type 1, Supermarket Type 2 and Food Mart
- **Product_Store_Sales_Total** - total revenue generated by the sale of that particular product in that particular store

# **Installing and Importing the necessary libraries**
"""

#Installing the libraries with the specified versions
!pip install numpy==2.0.2 pandas==2.2.2 scikit-learn==1.6.1 matplotlib==3.10.0 seaborn==0.13.2 joblib==1.4.2 xgboost==2.1.4 requests==2.32.3 huggingface_hub==0.30.1 -q

"""**Note:**

- After running the above cell, kindly restart the notebook kernel (for Jupyter Notebook) or runtime (for Google Colab) and run all cells sequentially from the next cell.

- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook.
"""

import warnings
warnings.filterwarnings("ignore")

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# For splitting the dataset
from sklearn.model_selection import train_test_split

# Libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 100)


# Libraries different ensemble classifiers
from sklearn.ensemble import (
    BaggingRegressor,
    RandomForestRegressor,
    AdaBoostRegressor,
    GradientBoostingRegressor,
)
from xgboost import XGBRegressor
from sklearn.tree import DecisionTreeRegressor

# Libraries to get different metric scores
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    mean_absolute_percentage_error
)

# To create the pipeline
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline,Pipeline

# To tune different models and standardize
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler,OneHotEncoder

# To serialize the model
import joblib

# os related functionalities
import os

# API request
import requests

# for hugging face space authentication to upload files
from huggingface_hub import login, HfApi

"""# **Loading the dataset**"""

from google.colab import drive
drive.mount('/content/drive')

url = "/content/drive/My Drive/UTA_AI_ML/Final_Deployment/SuperKart.csv"
kart_data = pd.read_csv(url)

data = kart_data.copy()

"""# **Data Overview**"""

data.head()

data.tail()

print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")

data.info()

data.describe(include="all").T

data.duplicated().sum()

data.isnull().sum()

"""# **Exploratory Data Analysis (EDA)**

## Univariate Analysis
"""

def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,
        sharex=True,
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )

histogram_boxplot(data, "Product_Weight")

histogram_boxplot(data, "Product_Allocated_Area")

histogram_boxplot(data, "Product_MRP")

histogram_boxplot(data, "Product_Store_Sales_Total")

def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )
        else:
            label = p.get_height()

        x = p.get_x() + p.get_width() / 2
        y = p.get_height()

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )

    plt.show()

labeled_barplot(data, "Product_Type", perc=True)

labeled_barplot(data, "Product_Sugar_Content", perc=True)

labeled_barplot(data, "Store_Id", perc=True)

labeled_barplot(data, "Store_Size", perc=True)

labeled_barplot(data, "Store_Location_City_Type", perc=True)

labeled_barplot(data, "Store_Type", perc=True)

"""## Bivariate Analysis"""

cols_list = data.select_dtypes(include=np.number).columns.tolist()

plt.figure(figsize=(10, 5))
sns.heatmap(
    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()

plt.figure(figsize=[8, 6])
sns.scatterplot(x=data.Product_Allocated_Area, y=data.Product_Store_Sales_Total)
plt.show()

plt.figure(figsize=[8, 6])
sns.scatterplot(x=data.Product_Weight, y=data.Product_Store_Sales_Total)
plt.show()

plt.figure(figsize=[8, 6])
sns.scatterplot(x=data.Product_MRP, y=data.Product_Store_Sales_Total)
plt.show()

df_revenue1 = data.groupby(["Product_Type"], as_index=False)[
    "Product_Store_Sales_Total"
].sum()
plt.figure(figsize=[14, 8])
plt.xticks(rotation=90)
a = sns.barplot(x=df_revenue1.Product_Type, y=df_revenue1.Product_Store_Sales_Total)
a.set_xlabel("Product Types")
a.set_ylabel("Revenue")
plt.show()

df_revenue2 = data.groupby(["Product_Sugar_Content"], as_index=False)[
    "Product_Store_Sales_Total"
].sum()
plt.figure(figsize=[8, 6])
plt.xticks(rotation=90)
b = sns.barplot(
    x=df_revenue2.Product_Sugar_Content, y=df_revenue2.Product_Store_Sales_Total
)
b.set_xlabel("Product_Sugar_content")
b.set_ylabel("Revenue")
plt.show()

df_store_revenue = data.groupby(["Store_Id"], as_index=False)[
    "Product_Store_Sales_Total"
].sum()
plt.figure(figsize=[8, 6])
plt.xticks(rotation=90)
r = sns.barplot(
    x=df_store_revenue.Store_Id, y=df_store_revenue.Product_Store_Sales_Total
)
r.set_xlabel("Stores")
r.set_ylabel("Revenue")
plt.show()

df_revenue3 = data.groupby(["Store_Size"], as_index=False)[
    "Product_Store_Sales_Total"
].sum()
plt.figure(figsize=[8, 6])
plt.xticks(rotation=90)
c = sns.barplot(x=df_revenue3.Store_Size, y=df_revenue3.Product_Store_Sales_Total)
c.set_xlabel("Store_Size")
c.set_ylabel("Revenue")
plt.show()

df_revenue4 = data.groupby(["Store_Location_City_Type"], as_index=False)[
    "Product_Store_Sales_Total"
].sum()
plt.figure(figsize=[8, 6])
plt.xticks(rotation=90)
d = sns.barplot(
    x=df_revenue4.Store_Location_City_Type, y=df_revenue4.Product_Store_Sales_Total
)
d.set_xlabel("Store_Location_City_Type")
d.set_ylabel("Revenue")
plt.show()

df_revenue5 = data.groupby(["Store_Type"], as_index=False)[
    "Product_Store_Sales_Total"
].sum()
plt.figure(figsize=[8, 6])
plt.xticks(rotation=90)
e = sns.barplot(x=df_revenue5.Store_Type, y=df_revenue5.Product_Store_Sales_Total)
e.set_xlabel("Store_Type")
e.set_ylabel("Revenue")
plt.show()

plt.figure(figsize=(14, 8))
sns.heatmap(
    pd.crosstab(data["Product_Sugar_Content"], data["Product_Type"]),
    annot=True,
    fmt="g",
    cmap="viridis",
)
plt.ylabel("Product_Sugar_Content")
plt.xlabel("Product_Type")
plt.show()

plt.figure(figsize=(14, 8))
sns.heatmap(
    pd.crosstab(data["Store_Id"], data["Product_Type"]),
    annot=True,
    fmt="g",
    cmap="viridis",
)
plt.ylabel("Stores")
plt.xlabel("Product_Type")
plt.show()

plt.figure(figsize=[14, 8])
sns.boxplot(data = data, x = "Store_Id", y = "Product_MRP", hue = "Store_Id")
plt.xticks(rotation=90)
plt.title("Boxplot - Store_Id Vs Product_MRP")
plt.xlabel("Stores")
plt.ylabel("Product_MRP (of each product)")
plt.show()

plt.figure(figsize=[14, 8])
sns.boxplot(data = data, x = "Product_Type", y = "Product_MRP", hue = "Product_Type")
plt.xticks(rotation=90)
plt.title("Boxplot - Product_Type Vs Product_MRP")
plt.xlabel("Product_Type")
plt.ylabel("Product_MRP (of each product)")
plt.show()

"""# **Data Preprocessing**"""

data["Product_Id_char"].unique()

"""# **Model Building**"""

data.Product_Sugar_Content.replace(to_replace=["reg"], value=["Regular"], inplace=True)

data.Product_Sugar_Content.value_counts()

data["Product_Id_char"] = data["Product_Id"].str[:2]
data.head()

data["Product_Id_char"].unique()

data.loc[data.Product_Id_char == "FD", "Product_Type"].unique()

data.loc[data.Product_Id_char == "Product_Id_char", "Product_Type"].unique()

data.loc[data.Product_Id_char == "Product_Id_char", "Product_Type"].unique()

data["Store_Age_Years"] = 2025 - data.Store_Establishment_Year

perishables = [
    "Dairy",
    "Meat",
    "Fruits and Vegetables",
    "Breakfast",
    "Breads",
    "Seafood",
]

def change(x):
    if x in perishables:
        return "Perishables"
    else:
        return "Non Perishables"

data['Product_Type_Category'] = data['Product_Type'].apply(change)

data.head()

data = data.drop(["Product_Id","Store_Id","Product_Type","Store_Establishment_Year"], axis=1)

X = data.drop("Product_Store_Sales_Total", axis=1)
y = data["Product_Store_Sales_Total"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=.75, random_state=1, shuffle=True
)

X_train.shape, X_test.shape

categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()
categorical_features

preprocessor = make_column_transformer(
    (Pipeline([('encoder', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)
)

"""## Define functions for Model Evaluation"""

# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mean_absolute_percentage_error(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf

"""The ML models to be built can be any two out of the following:
1. Decision Tree
2. Bagging
3. Random Forest
4. AdaBoost
5. Gradient Boosting
6. XGBoost
"""

dtree = DecisionTreeRegressor(random_state=1)
dtree = make_pipeline(preprocessor,dtree)
dtree.fit(X_train, y_train)

dtree_model_train_perf = model_performance_regression(dtree, X_train, y_train)
dtree_model_train_perf

dtree_model_test_perf = model_performance_regression(dtree, X_test, y_test)
dtree_model_test_perf

bagging_regressor = BaggingRegressor(random_state=1)
bagging_regressor = make_pipeline(preprocessor,bagging_regressor)
bagging_regressor.fit(X_train, y_train)

bagging_regressor_model_train_perf = model_performance_regression(bagging_regressor, X_train, y_train)
bagging_regressor_model_train_perf

bagging_regressor_model_test_perf = model_performance_regression(bagging_regressor, X_test, y_test)
bagging_regressor_model_test_perf

rf_estimator = RandomForestRegressor(random_state=1)
rf_estimator = make_pipeline(preprocessor,rf_estimator)
rf_estimator.fit(X_train, y_train)

rf_estimator_model_train_perf = model_performance_regression(rf_estimator, X_train, y_train)
rf_estimator_model_train_perf

rf_estimator_model_test_perf = model_performance_regression(rf_estimator, X_test, y_test)
rf_estimator_model_test_perf

ab_regressor = AdaBoostRegressor(random_state=1)
ab_regressor = make_pipeline(preprocessor,ab_regressor)
ab_regressor.fit(X_train, y_train)

ab_regressor_model_train_perf = model_performance_regression(ab_regressor, X_train, y_train)
ab_regressor_model_train_perf

ab_regressor_model_test_perf = model_performance_regression(ab_regressor, X_test, y_test)
ab_regressor_model_test_perf

gb_estimator = GradientBoostingRegressor(random_state=1)
gb_estimator = make_pipeline(preprocessor,gb_estimator)
gb_estimator.fit(X_train, y_train)

gb_estimator_model_train_perf = model_performance_regression(gb_estimator, X_train, y_train)
gb_estimator_model_train_perf

gb_estimator_model_test_perf = model_performance_regression(gb_estimator, X_test, y_test)
gb_estimator_model_test_perf

xgb_estimator = XGBRegressor(random_state=1)
xgb_estimator = make_pipeline(preprocessor,xgb_estimator)
xgb_estimator.fit(X_train, y_train)

xgb_estimator_model_train_perf = model_performance_regression(xgb_estimator, X_train, y_train)
xgb_estimator_model_train_perf

xgb_estimator_model_test_perf = model_performance_regression(xgb_estimator, X_test, y_test)
xgb_estimator_model_test_perf

models_train_comp_df = pd.concat(
    [
        dtree_model_train_perf.T,
        bagging_regressor_model_train_perf.T,
        rf_estimator_model_train_perf.T,
        ab_regressor_model_train_perf.T,
        gb_estimator_model_train_perf.T,
        xgb_estimator_model_train_perf.T
    ],
    axis=1,
)
models_train_comp_df.columns = [
   "Decision tree","Bagging Regressor","Random Forest Model","AdaBoost Regressor","Gradient Boosting Regressor","XGBoost Regressor"
]

models_train_comp_df

models_test_comp_df = pd.concat(
    [
        dtree_model_test_perf.T,
        bagging_regressor_model_test_perf.T,
        rf_estimator_model_test_perf.T,
        ab_regressor_model_test_perf.T,
        gb_estimator_model_test_perf.T,
        xgb_estimator_model_test_perf.T
    ],
    axis=1,
)
models_test_comp_df.columns = [
   "Decision tree","Bagging Regressor","Random Forest Model","AdaBoost Regressor","Gradient Boosting Regressor","XGBoost Regressor"
]

models_test_comp_df

"""# **Model Performance Improvement - Hyperparameter Tuning**

**All the models perform very close to each other. Looking at this we can try hypertuning RandomForest and XGBoost**
"""

rf_tuned = RandomForestRegressor(random_state=1)
rf_tuned = make_pipeline(preprocessor,rf_tuned)


parameters = {
     "randomforestregressor__max_depth": [3,4,5,6],
     "randomforestregressor__max_features":['sqrt','log2',None],
     "randomforestregressor__n_estimators": [50, 75, 100, 125, 150],
}


grid_obj = GridSearchCV(rf_tuned, parameters, scoring=r2_score, cv=3, n_jobs = -1)
grid_obj = grid_obj.fit(X_train, y_train)


rf_tuned = grid_obj.best_estimator_


rf_tuned.fit(X_train, y_train)

rf_tuned_model_train_perf = model_performance_regression(rf_tuned, X_train, y_train)
rf_tuned_model_train_perf

rf_tuned_model_test_perf = model_performance_regression(rf_tuned, X_test, y_test)
rf_tuned_model_test_perf

xgb_tuned = XGBRegressor(random_state=1)
xgb_tuned = make_pipeline(preprocessor,xgb_tuned)


parameters = {
    'xgbregressor__n_estimators': [50, 100, 150, 200],    # number of trees to build
    'xgbregressor___max_depth': [2, 3, 4],    # maximum depth of each tree
    'xgbregressor___colsample_bytree': [0.4, 0.5, 0.6],    # percentage of attributes to be considered (randomly) for each tree
    'xgbregressor___colsample_bylevel': [0.4, 0.5, 0.6],    # percentage of attributes to be considered (randomly) for each level of a tree
    'xgbregressor___learning_rate': [0.01, 0.05, 0.1],    # learning rate
    'xgbregressor___reg_lambda': [0.4, 0.5, 0.6],    # L2 regularization factor
}

# Run the grid search
grid_obj = GridSearchCV(xgb_tuned, parameters, scoring=r2_score, cv=3, n_jobs = -1)
grid_obj = grid_obj.fit(X_train, y_train)

xgb_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
xgb_tuned.fit(X_train, y_train)

xgb_tuned_model_train_perf = model_performance_regression(xgb_tuned, X_train, y_train)
xgb_tuned_model_train_perf

xgb_tuned_model_test_perf = model_performance_regression(xgb_tuned, X_test, y_test)
xgb_tuned_model_test_perf

"""# **Model Performance Comparison, Final Model Selection, and Serialization**"""

models_train_comp_df = pd.concat(
    [rf_estimator_model_train_perf.T,rf_tuned_model_train_perf.T,
    xgb_estimator_model_train_perf.T,xgb_tuned_model_train_perf.T],
    axis=1,
)

models_train_comp_df.columns = [
    "Random Forest Estimator",
    "Random Forest Tuned",
    "XGBoost",
    "XGBoost Tuned"
]

print("Training performance comparison:")
models_train_comp_df

# Testing performance comparison

models_test_comp_df = pd.concat(
    [rf_estimator_model_test_perf.T,rf_tuned_model_test_perf.T,
    xgb_estimator_model_test_perf.T,xgb_tuned_model_test_perf.T],
    axis=1,
)

models_test_comp_df.columns = [
    "Random Forest Estimator",
    "Random Forest Tuned",
    "XGBoost",
    "XGBoost Tuned"
]

print("Testing performance comparison:")
models_test_comp_df

(models_train_comp_df - models_test_comp_df).iloc[2]

os.makedirs("backend_files", exist_ok=True)

saved_model_path = "backend_files/supermarket_price_prediction_model.joblib"

joblib.dump(xgb_tuned, saved_model_path)

print(f"Model saved successfully at {saved_model_path}")

saved_model = joblib.load("backend_files/supermarket_price_prediction_model.joblib")


print("Model loaded successfully.")

saved_model

saved_model.predict(X_test)

"""# **Deployment - Backend**

## Flask Web Framework
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile backend_files/app.py
# 
# 
# import numpy as np
# import joblib  # For loading the serialized model
# import pandas as pd  # For data manipulation
# from flask import Flask, request, jsonify  # For creating the Flask API
# 
# # Initialize Flask app with a name
# superkart_api = Flask("my Super Kart")
# 
# # Load the trained churn prediction model
# model = joblib.load("supermarket_price_prediction_model.joblib")
# 
# # Define a route for the home page
# @superkart_api.get('/')
# def home():
#     return "Welcome to SuperKart prediction model"
# 
# 
# @superkart_api.post('/v1/predict')
# def predict_sales():
#     # Get JSON data from the request
#     data = request.get_json()
# 
#     expected_columns = [
#         'Product_Weight',
#         'Product_Sugar_Content',
#         'Product_Allocated_Area',
#         'Product_MRP',
#         'Store_Size',
#         'Store_Location_City_Type',
#         'Store_Type',
#         'Product_Id_char',
#         'Store_Age_Years',
#         'Product_Type_Category'
#     ]
# 
# 
#     sample = {col: data[col] for col in expected_columns}
# 
# 
#     input_data = pd.DataFrame([sample])[expected_columns]
# 
#     # Make a churn prediction using the trained model
#     prediction = model.predict(input_data).tolist()[0]
# 
#     # Return the prediction as a JSON response
#     return jsonify({'Sales': prediction})
# 
# 
# # Run the Flask app in debug mode
# if __name__ == '__main__':
#     superkart_api.run(debug=True)

"""## Dependencies File"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile backend_files/requirements.txt
# pandas==2.2.2
# numpy==2.0.2
# scikit-learn==1.6.1
# seaborn==0.13.2
# joblib==1.4.2
# xgboost==2.1.4
# joblib==1.4.2
# Werkzeug==2.2.2
# flask==2.2.2
# gunicorn==20.1.0
# requests==2.32.3
# uvicorn[standard]
# streamlit==1.43.2

"""## Dockerfile"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile backend_files/Dockerfile
# FROM python:3.9-slim
# 
# # Set the working directory inside the container
# WORKDIR /app
# 
# # Copy all files from the current directory to the container's working directory
# COPY . .
# 
# # Install dependencies from the requirements file without using cache to reduce image size
# RUN pip install --no-cache-dir --upgrade -r requirements.txt
# 
# # Define the command to start the application using Gunicorn with 4 worker processes
# # - `-w 4`: Uses 4 worker processes for handling requests
# # - `-b 0.0.0.0:7860`: Binds the server to port 7860 on all network interfaces
# # - `app:app`: Runs the Flask app (assuming `app.py` contains the Flask instance named `app`)
# CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:7860", "app:superkart_api"]

"""## Setting up a Hugging Face Docker Space for the Backend"""

from huggingface_hub import login

login(token="xxxx")
from huggingface_hub import create_repo

try:
    create_repo("TechCoder20/PricePredictionBackend",
        repo_type="space",
        space_sdk="docker",
        private=False
    )
except Exception as e:
    if "RepositoryAlreadyExistsError" in str(e):
        print("Repository already exists. Skipping creation.")
    else:
        print(f"Error creating repository: {e}")

"""## Uploading Files to Hugging Face Space (Docker Space)"""

access_key = "xxxx"
repo_id = "TechCoder20/PricePredictionBackend"


login(token=access_key)


api = HfApi()


api.upload_folder(
    folder_path="/content/backend_files",
    repo_id=repo_id,
    repo_type="space",
)

"""# **Deployment - Frontend**

## Points to note before executing the below cells
- Create a Streamlit space on Hugging Face by following the instructions provided on the content page titled **`Creating Spaces and Adding Secrets in Hugging Face`** from Week 1

## Streamlit for Interactive UI
"""

os.makedirs("frontend_files", exist_ok=True)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile frontend_files/app.py
# 
# import streamlit as st
# import requests
# 
# st.title("Super Kart sales forecast ")
# 
# Product_Weight = st.number_input("Product Weight", min_value=0.0, value=12.66, key="product_weight")
# Product_Sugar_Content = st.selectbox("Product Sugar Content", ["Low Sugar", "Regular", "No Sugar"], key="product_sugar_content")
# Product_Allocated_Area = st.number_input("Product Allocated Area", min_value=0.0, value=12.66, key="product_allocated_area")
# Product_MRP = st.number_input("Product MRP", min_value=0.0, value=0.0, key="product_mrp")
# Store_Size = st.selectbox("Store Size", ["High", "Medium", "Low"], key="store_size")
# Store_Location_City_Type = st.selectbox("Store Location City Type", ["Tier 1", "Tier 2", "Tier 3"], key="store_location_city_type")
# Store_Type = st.selectbox("Store Type", ["Departmental Store", "Supermarket Type 1", "Supermarket Type 2","Food Mart"], key="store_type")
# Product_Id_char = st.selectbox("Product ID Character", ["FD", "DR", "NC"], key="product_id_char")
# Store_Age_Years = st.number_input("Store Age (in years)", min_value=0, value=25, key="store_age_years")
# Product_Type_Category = st.selectbox("Product Type Category", ["Perishables", "Non Perishables"])
# 
# product_data = {
#     "Product_Weight": Product_Weight,
#     "Product_Sugar_Content": Product_Sugar_Content,
#     "Product_Allocated_Area": Product_Allocated_Area,
#     "Product_MRP": Product_MRP,
#     "Store_Size": Store_Size,
#     "Store_Location_City_Type": Store_Location_City_Type,
#     "Store_Type": Store_Type,
#     "Product_Id_char": Product_Id_char,
#     "Store_Age_Years": Store_Age_Years,
#     "Product_Type_Category": Product_Type_Category
# }
# 
# if st.button("Predict", type='primary'):
#     response = requests.post("https://TechCoder20-PricePredictionBackend.hf.space/v1/predict", json=product_data)
#     if response.status_code == 200:
#         result = response.json()
#         predicted_sales = result["Sales"]
#         st.write(f"Predicted Product Store Sales Total: ${predicted_sales:.2f}")
#     else:
#         st.error("Error in API request")

"""## Dependencies File"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile frontend_files/requirements.txt
# requests==2.32.3
# streamlit==1.45.0

"""## DockerFile"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile frontend_files/Dockerfile
# # Use a minimal base image with Python 3.9 installed
# FROM python:3.9-slim
# 
# # Set the working directory inside the container to /app
# WORKDIR /app
# 
# # Copy all files from the current directory on the host to the container's /app directory
# COPY . .
# 
# # Install Python dependencies listed in requirements.txt
# RUN pip3 install -r requirements.txt
# 
# # Define the command to run the Streamlit app on port 8501 and make it accessible externally
# CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0", "--server.enableXsrfProtection=false"]
# 
# # NOTE: Disable XSRF protection for easier external access in order to make batch predictions

"""## Uploading Files to Hugging Face Space (Streamlit Space)"""

access_key = "xxxxxxx"
repo_id = "TechCoder20/PricePredictionFrontEnd"


login(token=access_key)


api = HfApi()


api.upload_folder(
    folder_path="frontend_files",
    repo_id=repo_id,
    repo_type="space",
)

# added this to test the apis. Please ignore
product_data = {
    "Product_Weight": 123,
    "Product_Sugar_Content": "Regular",
    "Product_Allocated_Area": 890,
    "Product_MRP": 120,
    "Store_Size": "High",
    "Store_Location_City_Type": "Tier 1",
    "Store_Type": "Departmental Store",
    "Product_Id_char": "FD",
    "Store_Age_Years": 32,
    "Product_Type_Category": "Perishables"
}
response = requests.post("https://TechCoder20-PricePredictionBackend.hf.space/v1/predict", json=product_data)
if response.status_code == 200:
  result = response.json()
  predicted_sales = result["Sales"]
  print(predicted_sales)

"""# **Actionable Insights and Business Recommendations**

Frontend can be access via: https://huggingface.co/spaces/TechCoder20/PricePredictionFrontEnd

**Actionabels Insights:**
- Product MRP and Sales: There is a positive correlation between Product_MRP and Product_Store_Sales_Total. This suggests that products with higher maximum retail prices tend to generate higher sales revenue.
- Store Size and Sales: Stores with 'High' size tend to have significantly higher sales compared to 'Medium' and 'Low' sized stores.
- Store Location City Type and Sales: Stores in 'Tier 1' cities generally have higher sales revenue than those in 'Tier 2' and 'Tier 3' cities
-Store Type and Sales: 'Supermarket Type 1' stores generate the highest sales, followed by 'Supermarket Type 2', 'Departmental Store', and then 'Food Mart'.
- Product Type and Sales: Certain product types like 'Fruits and Vegetables', 'Snack Foods', and 'Household' items seem to contribute significantly to overall revenue.
Product Weight and Sales: Product weight does not seem to have a significant linear relationship with sales.

**Business Recommendations:**
- Focus on High-Value Products: Given the correlation between Product_MRP and sales, SuperKart could consider strategies to promote higher-priced items or explore opportunities to introduce more premium products in categories that are performing well.
- Optimize Store Size and Location: Prioritize expanding or opening new stores in 'Tier 1' cities and ensuring that new stores, where feasible, are of 'High' size. For existing 'Medium' and 'Low' sized stores in Tier 2 and Tier 3 cities, explore ways to optimize space utilization and product mix to boost sales.
- Leverage Supermarket Type 1 Success: Analyze the success factors of 'Supermarket Type 1' stores and try to replicate these strategies in other store types where applicable. This could involve optimizing store layout, product variety, and marketing efforts.
- Further Investigate Low-Performing Products/Stores: Identify products or stores with consistently low sales and delve deeper to understand the underlying reasons. This could involve analyzing factors like competition, local demographics, supply chain issues, or marketing effectiveness.
- Monitor Product Sugar Content: While the analysis of Product_Sugar_Content and sales didn't reveal strong patterns, it's important to continue monitoring this attribute as consumer preferences and health trends can influence sales.
- Utilize the Forecasting Model: The developed forecasting model can be integrated into inventory management systems and used to predict future sales for individual products in each store. This will help in optimizing stock levels, reducing waste, and ensuring product availability.

### Technical Skills Covered

**Programming and Libraries:**
*   **Python:** The entire project is developed using Python.
*   **Pandas:** Used for data manipulation, cleaning, and analysis.
*   **NumPy:** Used for numerical operations.
*   **Scikit-learn:** Utilized for building and evaluating machine learning models (Decision Tree, Bagging, Random Forest, AdaBoost, Gradient Boosting, XGBoost), and for data preprocessing (OneHotEncoder, train_test_split).
*   **Matplotlib & Seaborn:** Used for data visualization to understand data distributions and relationships.
*   **Joblib:** Used for model serialization and deserialization.
*   **XGBoost:** A specific and powerful gradient boosting library used for building a predictive model.
*   **Flask:** Used to create the backend API for the machine learning model.
*   **Streamlit:** Used to build the interactive frontend web application.
*   **Requests:** Used for making HTTP requests to the backend API from the frontend.
*   **Hugging Face Hub:** Used for deploying and hosting the machine learning model and the web application.

**Machine Learning:**
*   **Supervised Learning:** The project focuses on a regression task to predict sales.
*   **Regression Models:**
    *   Decision Tree Regressor
    *   Bagging Regressor
    *   Random Forest Regressor
    *   AdaBoost Regressor
    *   Gradient Boosting Regressor
    *   XGBoost Regressor
*   **Model Evaluation:**
    *   Metrics: R-squared, Adjusted R-squared, Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE).
*   **Hyperparameter Tuning:**
    *   GridSearchCV is used to find the optimal hyperparameters for the models.
*   **Feature Engineering:**
    *   Creating new features like `Store_Age_Years` and `Product_Type_Category`.
    *   Extracting features from existing ones, like `Product_Id_char` from `Product_Id`.
*   **Data Preprocessing:**
    *   Handling categorical variables using One-Hot Encoding.
    *   Splitting data into training and testing sets.

**Deployment and MLOps:**
*   **API Development:**
    *   Building a RESTful API using Flask to serve the machine learning model.
*   **Containerization:**
    *   Creating a Dockerfile to containerize the backend application, ensuring a consistent environment for deployment.
*   **Cloud Deployment:**
    *   Deploying the backend (Flask API) and frontend (Streamlit app) to Hugging Face Spaces.
*   **CI/CD (Implied):**
    *   The use of `requirements.txt` and a Dockerfile are fundamental practices in CI/CD pipelines for managing dependencies and environments.

**Data Analysis and Visualization:**
*   **Exploratory Data Analysis (EDA):**
    *   Univariate and Bivariate analysis to understand the data and relationships between variables.
*   **Data Visualization:**
    *   Creating various plots like histograms, boxplots, bar plots, scatter plots, and heatmaps to visualize data.

**Software Engineering Practices:**
*   **Modular Code:**
    *   Writing functions for repetitive tasks like model evaluation and plotting.
*   **Version Control (Implied):**
    *   The use of a structured project with separate files for the backend and frontend suggests an understanding of version control principles.

*   **Exploratory Data Analysis (EDA)**
*   **Data Pre-processing**
*   **Model Building:**
    *   Decision Tree
    *   Bagging
    *   Random Forest
    *   AdaBoost
    *   Gradient Boosting
    *   XGBoost
*   **Model Performance Evaluation and Improvement**
*   **Backend Development (Flask API)**
*   **Hugging Face Deployment**
*   **Business Recommendations**
"""